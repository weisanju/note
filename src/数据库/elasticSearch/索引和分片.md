### 概念定义

* 集群(cluster):由一个或多个节点组成, 并通过集群名称与其他集群进行区分
* 节点(node):单个ElasticSearch实例. 通常一个节点运行在一个隔离的容器或虚拟机中
* 索引(index):数据的 逻辑命名空间，分片的逻辑组合
* 分片(shard):存储数据的最小物理单元
* 副本(replica):主分片的一个副本



### 副本

本文中不会对ElasticSearch的副本做详细阐述. 如果想单独了解可参考[这篇文章](https://qbox.io/blog/announcing-replicated-elasticsearch-clusters).

副本对搜索性能非常重要, 同时用户也可在任何时候添加或删除副本. 正如[另篇文章](https://qbox.io/blog/announcing-replicated-elasticsearch-clusters)所述, 额外的副本能给你带来更大的容量, 更高的呑吐能力及更强的故障恢复能力.



### 分片

一个分片 *shard* 就是 es中的最小工作单元

它只是保存了索引中的所有数据的一部分

每个分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎





### **分片是ES在进群中分发数据的关键**

1. 可以把分片想想成数据的容器。文档存储在分片中

2. 然后分片分配到集群中的节点上。当集群扩容或缩小，ES将会自动在节点间迁移分片，以使集群保持平衡
3. 分片可以是主分片，或者是副本分片
4. ES默认为一个索引创建5个主分片, 并分别为其创建一个副本分片. 也就是说每个索引都由5个主分片成本, 而每个主分片都相应的有一个copy.  如果磁盘空间不足 15%，则不分配 replica shard。磁盘空间不足 5%，则不再分配任何的 primary shard。
5. 索引的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。
6. 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供请求，比如搜索或者从别的shard取回文档。



### **主分片数无法修改**

1. **当索引创建完成的时候，主分片的数量就固定了**，但是复制分片的数量可以随时调整
2. 在集群运行中你无法调整分片设置. 既便以后你发现需要调整分片数量, 你也只能新建创建并对数据进行重新索引(reindex)(虽然reindex会比较耗时, 但至少能保证你不会停机).
3. 主分片的配置与硬盘分区很类似, 在对一块空的硬盘空间进行分区时, 会要求用户先进行数据备份, 然后配置新的分区, 最后把数据写到新的分区上.



### **过度分配**

1. 稍有富余是好的, 但过度分配分片却是大错特错. 具体定义多少分片很难有定论, 取决于用户的数据量和使用方式
2. 每个分片都是有额外的成本的:
   1. 每个分片本质上就是一个Lucene索引, 因此会消耗相应的文件句柄, 内存和CPU资源
   2. 每个搜索请求会调度到索引的每个分片中. 如果分片分散在不同的节点倒是问题不太. 但当分片开始竞争相同的硬件资源时, 性能便会逐步下降
   3. ES使用[词频统计来计算相关性](https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html). 当然这些统计也会分配到各个分片上. 如果在大量分片上只维护了很少的数据, 则将导致最终的文档相关性较差
   4. 尽量保证同类数据 分布到 相同的分片
3. 如果你真的担心数据的快速增长, 我们建议你多关心这条限制 [ElasticSearch推荐的最大JVM堆空间](https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html)是30~32G,
   1. 所以把你的分片最大容量限制为30GB, 然后再对分片数量做合理估算. 例如, 你认为你的数据能达到200GB, 我们推荐你最多分配7到8个分片.
4. 总之, 不要现在就为你可能在三年后才能达到的10TB数据做过多分配. 如果真到那一天, 你也会很早感知到性能变化的.
5. 对大数据集, 我们非常鼓励你为索引多分配些分片--当然也要在合理范围内. 上面讲到的每个分片最好不超过30GB的原则依然使用

在开始阶段, 一个好的方案是根据你的节点数量按照1.5~3倍的原则来创建分片

例如,如果你有3个节点, 则推荐你创建的分片数最多不超过9(3x3)个



随着数据量的增加,如果你通过[集群状态API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html?q=cluster stat)发现了问题,或者遭遇了性能退化,则只需要增加额外的节点即可. ES会自动帮你完成分片在不同节点上的分布平衡.



再强调一次, 虽然这里我们暂未涉及副本节点的介绍, 但上面的指导原则依然使用: 是否有必要在每个节点上只分配一个索引的分片. 

另外, 如果给每个分片分配1个副本, 你所需的节点数将加倍. 如果需要为每个分片分配2个副本, 则需要3倍的节点数. 更多详情可以参考[基于副本的集群](http://blog.qbox.io/announcing-replicated-elasticsearch-clusters).



### Logstash

日志场景就是 基于日期的索引需求, 并且对索引数据的搜索场景非常少. 

也许这些索引量将达到成百上千, 但每个索引的数据量只有1GB甚至更小.

对于这种类似场景, 我建议你只需要为索引分配1个分片.



如果使用*ES的默认配置(5个分片*, 并且使用Logstash按天生成索引, 那么6个月下来, 你拥有的分片数将达到890个

再多的话, 你的集群将难以工作--除非你提供了更多(例如15个或更多)的节点.

想一下, 大部分的Logstash用户并不会频繁的进行搜索, 甚至每分钟都不会有一次查询. 所以这种场景, 推荐更为经济使用的设置. 在这种场景下, 搜索性能并不是第一要素, 所以并不需要很多副本. 维护单个副本用于数据冗余已经足够. 不过数据被不断载入到内存的比例相应也会变高.



如果你的索引只需要一个分片, 那么使用Logstash的配置可以在3节点的集群中维持运行6个月. 当然你至少需要使用4GB的内存, 不过建议使用8GB, 因为在多数据云平台中使用8GB内存会有明显的网速以及更少的资源共享.

再次声明, 数据分片也是要有相应资源消耗,并且需要持续投入



当索引拥有较多分片时, 为了组装查询结果, ES必须单独查询每个分片(当然并行的方式)并对结果进行合并. 所以高性能IO设备(SSDs)和多核处理器无疑对分片性能会有巨大帮助. 尽管如此, 你还是要多关心数据本身的大小,更新频率以及未来的状态. 在分片分配上并没有绝对的答案, 只希望你能从本文的讨论中受益.



**一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519 个 docs**



[参考链接](https://qbox.io/blog/optimizing-elasticsearch-how-many-shards-per-index)



### 分片设计推荐

1. 每一个分片数据文件小于30GB    
2. 每一个索引中的一个分片对应一个节点    
3. 节点数大于等于分片数



### 分片数量计算

**假如需要300G文件大小**

1. 至少需要 10个分片，每个分片位于独立的节点 则至少需要10个节点

   **SN(分片数) = IS(索引大小) / 30**

2. **NN(节点数) = SN(分片数) + MNN(主节点数[无数据]) + NNN(负载节点数)**



### 分片查询

#### **randomizeacross shards**

随机选择分片查询数据，es的默认方式

#### **_local**

优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。

#### **_primary**

只在主分片中查询不去副本查，一般数据完整。

#### **_primary_first**

优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。



#### **_only_node**

只在指定id的节点中的分片中查询，数据可能不完整。

#### **_prefer_node**

优先在指定你给节点中查询，一般数据完整。

#### **_shards**

在指定分片中查询，数据可能不完整。

#### **_only_nodes**

可以自定义去指定的多个节点查询，es不提供此方式需要改源码。





```java
 /** 
         * 指定分片 查询 
         */  
        @Test  
        public void testPreference()  
        {  
            SearchResponse searchResponse = transportClient.prepareSearch(index)  
                    .setTypes("add")  
                    //.setPreference("_local")  
                    //.setPreference("_primary")  
                    //.setPreference("_primary_first")  
                    //.setPreference("_only_node:ZYYWXGZCSkSL7QD0bDVxYA")  
                    //.setPreference("_prefer_node:ZYYWXGZCSkSL7QD0bDVxYA")  
                    .setPreference("_shards:0,1,2")  
                    .setQuery(QueryBuilders.matchAllQuery()).setExplain(true).get();  

            SearchHits hits = searchResponse.getHits();  
            System.out.println(hits.getTotalHits());  
            SearchHit[] hits2 = hits.getHits();  
            for(SearchHit h : hits2)  
            {  
                System.out.println(h.getSourceAsString());  
            }  
        }  
```





### 分片复制过程

我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点

新建索引和删除请求都是写操作，它们必须在主分片上成功完成才能赋值到相关的复制分片上

在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤：

1. 客户端给Node1 发送新建、索引或删除请求。
2. 节点使用文档的_id确定文档属于分片0.转发请求到Node3，分片0位于这个节点上。
3. Node3在主分片上执行请求，如果成功，它转发请求到相应的位于Node1和Node2的复制节点上
4. 当所有的复制节点报告成功，Node3报告成功到请求的节点，请求的节点再报告给客户端。
5. 客户端接收到成功响应的时候，文档的修改已经被用于主分片和所有的复制分片，修改生效了。





**ES分片复制**

* 复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。
* 如果你设置replication为async,请求在主分片上被执行后就会返回给客户端。它依旧会转发给复制节点，但你将不知道复制节点成功与否。





### 节点类型

- master 节点： 集群中的一个节点会被选为 master 节点，它将负责管理集群范畴的变更，例如创建或删除索引，添加节点到集群或从集群中删除节点。master 节点无需参与文档层面的变更和搜索，这意味着仅有一个 master 节点并不会因流量增长而成为瓶颈。任意一个节点都可以成为 master 节点。
- data 节点： 持有数据和倒排索引。默认情况下，每个节点都可以通过设定配置文件 elasticsearch.yml 中的 node.data 属性为 true (默认) 成为数据节点。如果需要一个专门的主节点 (一个节点既可以是 master 节点，同时也可以是 data 节点)，应将其 node.data 属性设置为 false。
- client 节点： 如果将 node.master 属性和 node.data 属性都设置为 false，那么该节点就是一个客户端节点，扮演一个负载均衡的角色，将到来的请求路由到集群中的各个节点。







### 分片与索引实战

#### 新建索引分片实例

```
PUT /testIndex
{
	"settings":{
		"number_of_shards":12,
		"number_of_replicas":1
	}
}
```

#### 调整分片数

```
PUT /testIndex/_settings
{
	"number_of_replicas":2
}
```

#### 检查分片信息

```
GET _cat/shards?v
```

#### 检查索引信息

```
GET _cat/indices
```



#### 设置磁盘水位

```
PUT _cluster/settings 
{ 
	"transient": { 
		"cluster.routing.allocation.disk.watermark.low": "90%", 
		"cluster.routing.allocation.disk.watermark.high": "5gb"
	 }
 }

```

1. ES 默认当磁盘空间不足 15%时，会禁止分配 replica shard。可以动态调整 ES 对磁盘空间的要求限制；
2. 配置磁盘空间限制的时候，要求low必须比 high 大，可以使用百分比或 gb 的方式设置，且ES要求low至少满足磁盘 95%的容量。
3. low - 对磁盘空闲容量的最低限制（默认85%）；
4. high - 对磁盘空闲容量的最高限制（默认90%，极限值95%）
5. low 为 50gb。high 为 10gb。则当磁盘空闲容量不足 50gb 时停止分配 replica shard。 当磁盘空闲容量不足 10gb 时，停止分配 shard，并将应该在当前结点中分配的 shard 分配 到其他结点中；

#### 查看集群健康状态

```
GET _cluster/health

{
  "cluster_name" : "elasticsearch",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 8,
  "active_shards" : 16,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
```



### 索引未分配原因

1. INDEX_CREATED：由于创建索引的API导致未分配。
2. CLUSTER_RECOVERED：由于完全集群恢复导致未分配。
3. INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。
4. DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。
5. NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。
6. EXISTING_INDEX_RESTORED：由于恢复到已关闭的索引导致未分配。
7. REPLICA_ADDED：由于显式添加副本分片导致未分配。
8. ALLOCATION_FAILED ：由于分片分配失败导致未分配。
9. NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。
10. REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。
11. REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。
12. REALLOCATED_REPLICA：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。



#### 查看具体未分配原因

```
GET /_cluster/allocation/explain

GET _cat/indices?v&health=red


GET /_cat/shards?v&h=n,index,shard,prirep,state,sto,sc,unassigned.reason,unassigned.details
```



#### 尝试重新分配失败的分片

```bash
POST /_cluster/reroute?retry_failed=true
```

默认索引的尝试次数为5

```bash
PUT /indexname/_settings
{
  "index": {
    "allocation": {
      "max_retries": 20
    }
  }
}
```





#### 将副本分片提升为主分片

如果确定了主分片已经损坏，可以尝试将副本分片提升为主(会丢部分数据)：

```cpp
POST /_cluster/reroute?pretty
{
  "commands": [
    {
      "allocate_stale_primary": {
        "index": "indexname",//索引名
        "shard": 3,//操作的分片id
        "node": "node1",//此分片副本位于的节点
        "accept_data_loss": true//提示数据可能会丢失
      }
    }
  ]
}
```

此方案存在一个问题是需要提前知道此分片的副本位于哪个节点用以指定，可以通过如果api获取副本分片位置：

```undefined
GET _shard_stores?pretty
GET indexname/_shard_stores?pretty
```

#### 手动判断目录

判断当前es进程使用的数据目录:通过pid和yml配置的目录去匹配，如data

```kotlin
ll /proc/pid/fd |grep data
```

如果索引损坏导致api失效，则需要人工去数据目录进行查找副本分片位置,目录结构如下:

```skotlin
data/nodes/0/indices/Z60wvPOWSP6Qbk79i757Vg/0
```

数据目录下**为节点号 -> 索引文件夹 -> 索引ID -> 分片号**



#### 将此分片置为空分片

1. 如果此分片的主副都已经损坏，则可将此分片置为空以保留索引其他分片数据：

2. ```json
   {
     "commands": [
       {
         "allocate_empty_primary": {
           "index": "indexname",//索引名
           "shard": 3,//操作的分片id
           "node": "node1",//空分片要分配的节点
           "accept_data_loss": true//提示数据可能会丢失
         }
       }
     ]
   }
   ```

3. 如果集群存在大量索引分片无法恢复，则可以使用脚本将全部分片置空,可以基于下面的脚本修改：

```bash
#!/bin/bash
master=$(curl -s 'http://localhost:9200/_cat/master?v' | grep -v ' ip ' | awk '{print $1}')
for index in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | awk '{print $1}' | sort | uniq); do
    for shard in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | grep $index | awk '{print $2}' | sort | uniq); do
        echo  $index $shard
        curl -XPOST -H 'Content-Type: application/json'  'http://localhost:9200/_cluster/reroute' -d '{
            "commands" : [ {
                  "allocate_empty_primary" : {
                      "index" : "'$index'",
                      "shard" : "'$shard'",
                      "node" : "'$master'",
                  "accept_data_loss" : true
                  }
                }
            ]
        }'
        sleep 1
    done
done
```


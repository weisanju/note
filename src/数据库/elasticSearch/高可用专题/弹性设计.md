## Designing for resilience

1. 像Elasticsearch这样的分布式系统被设计为：即使它们的某些组件发生故障也可以继续工作。
2. 只要有足够的连接良好的节点来接管其职责，如果Elasticsearch集群的某些节点不可用或断开连接，它就可以继续正常运行。

弹性集群的大小是有限度的。所有的Elasticsearch集群都要求:

- One [elected master node](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-quorums.html) node：一个选举的主节点
- At least one node for each [role](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html).  每个 role至少一个 节点
- At least one copy of every [shard](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/scalability.html). 每个分片至少一个备份

一个弹性的集群 需要 为每一个 必要的集群组件 提供冗余，这意味着一个弹性集群需要

- At least three master-eligible nodes：至少三个 有资格的 master
- At least two nodes of each role：每个 role至少两个节点
- At least two copies of each shard (one primary and one or more replicas, unless the index is a [searchable snapshot index](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html))

1. 弹性集群需要三个  可以当选主节点的节点。这样 一旦其中一个宕机，剩下的两个 任然可以组成大多数，可以成功获得选举
2. 同样的，每个role的 节点冗余 意味着某个节点宕机了，另一个节点可以顶替其职责
3. 最后每个分片至少 需要 两个 copy 。如果其中一个 copy失败了，另一个则顶上
4. 并且，es会自动 在剩余的节点 上 重新构建 副本分片，以确保 能及时恢复 集群的健康
5. 故障会暂时降低 集群的总容量，故障后，群集必须执行额外的后台活动以使其恢复健康。即使某些节点出现故障，您也应确保群集具有处理工作负载的能力。



根据您的需求和预算，Elasticsearch集群可以由单个节点、数百个节点或两者之间的任意数量组成。在设计较小的集群时，通常应该专注于使其能够适应单节点故障。较大集群的设计者还必须考虑多个节点同时发生故障的情况。以下页面给出了构建各种规模的弹性集群的一些建议:

- [Resilience in small clusters](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/high-availability-cluster-small-clusters.html)
- [Resilience in larger clusters](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/high-availability-cluster-design-large-clusters.html)





## Resilience in small clusters

在较小的集群中，最重要的是要对单节点故障具有弹性。本节提供了一些指导，以使您的群集对单个节点的故障具有尽可能的弹性。

### One-node clusters

1. 如果您的集群由一个节点组成，则该单个节点必须执行所有操作。
2. 为了适应这一点，Elasticsearch默认为节点分配每个角色。
3. 单个节点群集没有弹性。如果节点出现故障，集群将停止工作
4. 默认情况下：对于一个 绿色的集群来说，至少需要一个 副本分片
5. 可以通过  设置 [`index.number_of_replicas`](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings) to `0` 来修改默认副本的数
6. 如果节点失败了，则唯一的办法是 从 快照中恢复
7. 由于它们无法抵御任何故障，因此我们不建议在生产中使用单节点群集。

### Two-node clusters

1. 如果是双节点，则两个节点最后都是 数据节点
2. 还要确保 每个 索引的副本数为1   [`index.number_of_replicas`](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings) 
3. 副本数可能会被  [index template](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html). [Auto-expand replicas](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings)（小集群不要使用）
4. 给其中一个节点 设置  `node.master: false`  ，这就意味着可以 清楚的知道 谁是主节点
5. 如果任一节点不可用，则选举将失败，因此您的群集无法可靠地容忍任一节点的丢失。

6. 默认情况下，每个节点都会被分配所有角色。我们建议您为两个节点分配除主资格之外的所有其他角色。如果一个节点发生故障，则另一个节点可以处理其任务。
7. 您可以使用弹性负载均衡器来平衡群集中节点之间的客户端请求。
8. 因为它对故障没有弹性，所以我们不建议在生产中部署两个节点的集群。



### Two-node clusters with a tiebreaker

1. 由于主节点的选举是基于大多数的，因此上述双节点集群可以容忍其一个节点的丢失，而另一个节点则不能容忍丢失
2. 您不能将  两个节点的群集，配置成 可以容忍任何一个节点的丢失，因为这在理论上是不可能的。
3. 您可能会期望，如果任一节点出现故障，则Elasticsearch可以选择其余节点作为主节点，但是无法分辨
   1. 节点的故障  或者
   2. 节点之间仅失去连接 这两者 的区别。
   3. 如果双方都能 进行独立的选举，这将可能导致脑裂 [split-brain problem](https://en.wikipedia.org/wiki/Split-brain_(computing)) 
4. Elasticsearch  直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点 才会选举成主节点，这可能会导致集群中没有主节点 直到 连接恢复
5. 解决办法是 增加第三个节点 ，使得这三个节点都可以选举主节点
6.  A [master election](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-quorums.html) 需要2/3投票， 这意味着集群可以容忍任何单个节点的丢失
7. 在两个原始节点彼此断开连接的情况下，第三个节点充当决胜局
8. 您可以通过使此节点成为 [dedicated voting-only master-eligible node](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node), 也称为 dedicated tiebreaker.因为它没有其他角色，所以投票节点不需要很高的性能配置 它不会执行任何搜索，也不会协调任何客户端请求，并且不能被选为群集的主服务器。
9. 如果您的三个节点中有两个是仅具有投票资格的主节点，则当选的主节点必须是第三个节点。然后，此节点成为单点故障。

10. 我们建议 分配 非 dedicated tiebreaker 节点 分配所有其他角色。这确保群集中的任何任务都可以由任一节点处理来创建冗余。
11. 您不应该向专用的tiebreaker节点发送任何客户端请求，也不能只向 其中一个或者连个节点发送请求，理想情况是 在非 tiebreaker 节点中保持负载均衡

带有额外的tiebreaker节点的两节点群集是适合生产部署的最小可能群集。

### Three-node clusters

1. 如果你有三个节点，推荐 都设置为 数据节点 [data nodes](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node) ，每个索引 至少有一个 副本

2. 您可能希望某些索引具有两个副本，以便每个节点在这些索引中具有每个分片的副本
3. 三个节点都是可当选主节点的，默认如此


### Clusters with more than three nodes

1. 如果有超过三个节点以上的集群，可以考虑 根据不同的职责 将分配不同的角色，这可以按需 扩缩资源
2. You can have as many [data nodes](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node), [ingest nodes](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html), [machine learning nodes](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#ml-node)
3. 建议每个角色 都有一部分 专有的 节点，这使您可以独立地扩展每个任务的资源。
4. 但是，将群集中 可以当选主节点的数量限制为三个是很好的做法。因为 主节点不会 像其他节点一样 扩缩，因为 集群总是选举他们其中的一个当做主节点
5. 如果主节点数太多，导致选举时间耗费太长。
6. 推荐专用的主节点，不要往主节点上发请求。因为如果主节点因为 其他任务 过载 会导致集群变得不稳定
7. 也可以配置 其中一个 可当选主节点的节点 为 [voting-only node](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node)  ，这样它就永远不能当选主节点，从而可以 担当 数据节点等其他节点功能，而它自己只是作为选举过程中的 tiebreaker

### Summary

The cluster will be resilient to the loss of any node as long as:

- The [cluster health status](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html) is `green`.
- There are at least two data nodes. 至少有两个数据节点
- Every index that is not a [searchable snapshot index](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html) has at least one replica of each shard, in addition to the primary. 分片至少有一个副本
- The cluster has at least three master-eligible nodes, as long as at least two of these nodes are not voting-only master-eligible nodes. 至少有三个可当选主节点的节点
- Clients are configured to send their requests to more than one node or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The [Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) service provides such a load balancer. 客户端的负载均衡



## Resilience in larger clusters

1. 节点共享一些公共基础设施 (例如电源或网络路由器) 并不罕见
2. 如果是这样，您应该为此基础架构的故障进行计划，并确保此类故障不会影响太多节点。
3. 通常的做法是将共享某些基础结构的所有节点分组为区域，并立即计划任何整个区域的故障。
4. 您集群的区域都应包含在单个数据中心内
5. Elasticsearch期望其节点到节点的连接是可靠的，并且具有低延迟和高带宽。数据中心之间的连接通常不符合这些期望
6. 尽管Elasticsearch在不可靠或缓慢的网络上表现正确，但它不一定表现最佳
7. 群集从网络分区完全恢复可能需要相当长的时间，因为它必须重新同步任何丢失的数据，并在分区恢复后重新平衡群集。
8. 如果您希望您的数据在多个数据中心可用，请在每个数据中心部署一个单独的集群，并  使用  [cross-cluster search](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cross-cluster-search.html) or [cross-cluster replication](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/xpack-ccr.html)  将集群连接在一起
9. 即使群集到群集的连接比每个群集内的网络可靠性差或速度慢，这些功能也被设计为性能良好。
10. 在失去整个区域的节点之后，设计正确的群集可能会起作用，但运行时容量会大大降低。在处理此类故障时，您可能需要配置额外的节点以恢复群集中可接受的性能。
11. 为了抵御全区域故障，重要的是在多个区域中存在每个分片的副本，这可以通过将数据节点放置在多个区域中并配置分片分配意识( [shard allocation awareness](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html).)来实现。您还应确保将客户端请求发送到多个区域中的节点。
12. 您应该考虑所有节点角色，并确保每个角色在两个或多个区域中冗余地拆分。
13. 例如, 使用 [ingest pipelines](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html) 或 machine learning, 您应该在两个或多个区域中有多个 ingest 或 machine learning  节点。
14. 但是，主控合格节点的放置需要更多的注意，因为弹性集群需要三个主控合格节点中的至少两个才能起作用。以下各节将探讨跨多个区域放置主控合格节点的选项。

### Two-zone clusters

1. 如果您有两个区域，您应该在每个区域中有不同数量的主控合格节点，以便具有更多节点的区域将包含其中的大多数，并且能够在另一个区域的丢失中幸存下来。

2. 例如，如果您有三个主控合格的节点，那么您可以将它们全部放在一个区域中，或者可以将两个放在一个区域中，第三个放在另一个区域中。
3. 您不应该在每个区域中放置相等数量的符合主条件的节点，如果在每个区域中放置相同数量的主控合格节点，则两个区域都没有自己的大部分。因此，群集可能无法幸免于任何一个区域的丢失。



### Two-zone clusters with a tiebreaker

1. 上述双区部署可以容忍其中一个区的丧失，但不能容忍另一个区的丧失，因为主选举是基于多数的

2. 您可能会想，如果任一区域失败，则Elasticsearch可以从其余区域中选择一个节点作为主节点，但是无法分辨出，远程区域的故障 和 区域之间仅仅失去连接这两者之间的区别。

3. 如果两个区域都独立运行 选举，那么 丧失连接 这可能会导致 [split-brain problem](https://en.wikipedia.org/wiki/Split-brain_(computing))  从而导致数据丢失
4. Elasticsearch避免了这种情况，并通过不选择来自任一区域的节点作为主节点来保护您的数据，直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点。这可能意味着在恢复连接之前根本没有master。
5. 您可以通过在两个区域中的每个区域中放置一个主控合格的节点，并在独立的第三区域中添加一个额外的主控合格的节点 来解决此问题。
6. 在两个原始区域彼此断开连接的情况下，额外的主控合格节点 充当tiebreaker ， The extra tiebreaker node should be a [dedicated voting-only master-eligible node](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node), 
7. 您应该使用  [shard allocation awareness](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html)  来确保每个区域中都有每个分片的副本。这意味着如果另一个区域出现故障，则任何一个区域都保持完全可用。
8. 所有主控合格节点 (包括仅投票节点) 都在发布群集状态更新的关键路径上。因此，这些节点需要**合理快速的持久性存储**以及与**群集其他节点的可靠，低延迟的网络连接**。如果在第三个独立区域中添加了tiebreaker节点，则必须确保它具有足够的资源并且与群集的其余部分具有良好的连接性。



### Clusters with three or more zones



1. 如果您有三个区域，那么每个区域中应该有一个主控合格节点
2. 如果您有三个以上的区域，则应选择三个区域，并在这三个区域中的每个区域中放置一个符合主条件的节点。
3. 这将意味着即使其中一个区域失败，群集仍然可以选择主服务器。
4. 与往常一样，您的索引应该至少有一个副本，以防节点出现故障
5. 使用  [shard allocation awareness](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html)  来限制 每个分片在 每个 区域的 副本分片数
6. 例如，如果您有一个配置了一个或两个副本的索引，则 allocation awareness 将确保碎片的副本与主副本位于不同的区域中
7. 这意味着如果一个区域失败，每个分片的副本仍然可用。这种碎片的可用性不会受到这种故障的影响。



### Summary

The cluster will be resilient to the loss of any zone as long as:

- The [cluster health status](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html) is `green`. 集群状态为绿色
- There are at least two zones containing data nodes. 至少两个包含数据节点的区域
- Every index that is not a [searchable snapshot index](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html) has at least one replica of each shard, in addition to the primary.：每个主分片 有 额外的副本 分片
- Shard allocation awareness is configured to avoid concentrating all copies of a shard within a single zone. Shard allocation awareness 避免将 所有副本集中在 单个区域 
- The cluster has at least three master-eligible nodes. At least two of these nodes are not voting-only master-eligible nodes, and they are spread evenly across at least three zones.
- Clients are configured to send their requests to nodes in more than one zone or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The [Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) service provides such a load balancer.
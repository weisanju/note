## PipeLineExample

In this example tutorial, you’ll use an [ingest pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html) to parse server logs in the [Common Log Format](https://en.wikipedia.org/wiki/Common_Log_Format) before indexing. 

The logs you want to parse look similar to this:



```log
212.87.37.154 - - [30/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\"
200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"
```



这些日志包含时间戳、ip地址和用户代理。您希望在Elasticsearch中赋予这三个项目自己的字段，以实现更快的搜索和可视化。您还想知道请求是从哪里来的。

1. In Kibana, open the main menu and click **Stack Management** > **Ingest Pipelines**.

2. Click **Create pipeline**.

3. Provide a name and description for the pipeline.

4. Add a [grok processor](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/grok-processor.html) to parse the log message:

   1. Click **Add a processor** and select the **Grok** processor type.

   2. Set **Field** to `message` and **Patterns** to the following [grok pattern](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/grok-basics.html):

   3. ```grok
      %{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}
      ```

   4. Click **Add** to save the processor.

   5. Set the processor description to `Extract fields from 'message'`.

5. Add processors for the timestamp, IP address, and user agent fields. Configure the processors as follows:

| Processor type                                               | Field        | Additional options                    | Description                                       |
| ------------------------------------------------------------ | ------------ | ------------------------------------- | ------------------------------------------------- |
| [**Date**](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/date-processor.html) | `@timestamp` | **Formats**: `dd/MMM/yyyy:HH:mm:ss Z` | `Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'` |
| [**GeoIP**](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/geoip-processor.html) | `source.ip`  | **Target field**: `source.geo`        | `Add 'source.geo' GeoIP data for 'source.ip'`     |
| [**User agent**](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/user-agent-processor.html) | `user_agent` |                                       | `Extract fields from 'user_agent'`                |

Your form should look similar to this:

The four processors will run sequentially:
Grok > Date > GeoIP > User agent
You can reorder processors using the arrow icons.

Alternatively, you can click the **Import processors** link and define the processors as JSON:

```js
{
  "processors": [
    {
      "grok": {
        "description": "Extract fields from 'message'",
        "field": "message",
        "patterns": ["%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}"]
      }
    },
    {
      "date": {
        "description": "Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'",
        "field": "@timestamp",
        "formats": [ "dd/MMM/yyyy:HH:mm:ss Z" ]
      }
    },
    {
      "geoip": {
        "description": "Add 'source.geo' GeoIP data for 'source.ip'",
        "field": "source.ip",
        "target_field": "source.geo"
      }
    },
    {
      "user_agent": {
        "description": "Extract fields from 'user_agent'",
        "field": "user_agent"
      }
    }
  ]

}
```

6. To test the pipeline, click **Add documents**.
7. In the **Documents** tab, provide a sample document for testing:

```js
[
  {
    "_source": {
      "message": "212.87.37.154 - - [05/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\""
    }
  }
]
```

8. Click **Run the pipeline** and verify the pipeline worked as expected.
9. If everything looks correct, close the panel, and then click **Create pipeline**.
   1. You’re now ready to index the logs data to a [data stream](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-streams.html).
10. Create an [index template](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html) with [data stream enabled](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-a-data-stream.html#create-index-template).

```console
PUT _index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500
}
```

11. Index a document with the pipeline you created.

```console
POST my-data-stream/_doc?pipeline=my-pipeline
{
  "message": "212.87.37.154 - - [05/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\""
}
```

12. To verify, search the data stream to retrieve the document. The following search uses [`filter_path`](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#common-options-response-filtering) to return only the [document source](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/mapping-source-field.html).

```console
GET my-data-stream/_search?filter_path=hits.hits._source
```
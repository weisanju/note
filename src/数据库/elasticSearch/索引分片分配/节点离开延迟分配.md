## Delaying allocation when a node leaves

当节点出于任何原因 (有意或其他原因) 离开群集时，主节点的反应是: 

1. 将副本分片提升到主，以替换节点上的任何主分片

2. 分配副本分片以替换丢失的副本 (假设有足够的节点)。•

3. 在剩余节点上均匀地重新平衡分片。

   

这些操作旨在通过确保尽快完全复制每个分片来保护群集免受数据丢失。



尽管我们限制了 并发 recoveries at the [node level](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/recovery.html) and at the [cluster level](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shards-allocation.html),

这种 “shard-shuffle” 仍然可以给集群带来很多额外的负载，如果丢失的节点可能很快返回，这可能是不必要的。想象一下这种情况:



- Node 5 loses network connectivity.
- The master promotes a replica shard to primary for each primary that was on Node 5.
- The master allocates new replicas to other nodes in the cluster.
- Each new replica makes an entire copy of the primary shard across the network.
- More shards are moved to different nodes to rebalance the cluster.
- Node 5 returns after a few minutes.
- The master rebalances the cluster by allocating shards to Node 5.



**延迟等待**

If the master had just waited for a few minutes, then the missing shards could have been re-allocated to Node 5 with the minimum of network traffic. 



This process would be even quicker for idle shards (shards not receiving indexing requests) which have been automatically [sync-flushed](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-synced-flush-api.html).



The allocation of replica shards which become unassigned because a node has left can be delayed with the `index.unassigned.node_left.delayed_timeout` dynamic setting, which defaults to `1m`.

This setting can be updated on a live index (or on all indices):

```console
PUT _all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m"
  }
}
```

With delayed allocation enabled, the above scenario changes to look like this:

启用了延迟分配，上述的流程变成以下

1. Node 5 loses network connectivity.
2. The master promotes a replica shard to primary for each primary that was on Node 5.
3. The master logs a message that allocation of unassigned shards has been delayed, and for how long.
4. The cluster remains yellow because there are unassigned replica shards.
5. Node 5 returns after a few minutes, before the `timeout` expires.
6. The missing replicas are re-allocated to Node 5 (and sync-flushed shards recover almost immediately).

This setting will not affect the promotion of replicas to primaries, nor will it affect the assignment of replicas that have not been assigned previously. In particular, delayed allocation does not come into effect after a full cluster restart. Also, in case of a master failover situation, elapsed delay time is forgotten (i.e. reset to the full initial delay).



### Cancellation of shard relocation

If delayed allocation times out, the master assigns the missing shards to another node which will start recovery. If the missing node rejoins the cluster, and its shards still have the same sync-id as the primary, shard relocation will be cancelled and the synced shard will be used for recovery instead.



For this reason, the default `timeout` is set to just one minute: even if shard relocation begins, cancelling recovery in favour of the synced shard is cheap.





### Monitoring delayed unassigned shards

The number of shards whose allocation has been delayed by this timeout setting can be viewed with the [cluster health API](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html):

```console
GET _cluster/health 
```

This request will return a `delayed_unassigned_shards` value.



### Removing a node permanently

If a node is not going to return and you would like Elasticsearch to allocate the missing shards immediately, just update the timeout to zero:

```console
PUT _all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "0"
  }
}
```

You can reset the timeout as soon as the missing shards have started to recover.


## Ingest pipelines



摄取管道(Ingest pipelines)允许您在索引之前对数据执行常见的转换。

例如，您可以使用管道删除字段，从文本中提取值并丰富数据。



管道由一系列称为处理器( [processors](https://www.elastic.co/guide/en/elasticsearch/reference/current/processors.html))的可配置任务组成。每个处理器按顺序运行，对传入文档进行特定更改。处理器运行后，Elasticsearch将转换后的文档添加到数据流或索引中。



您可以使用Kibana的 “摄取管道” 功能或摄取api创建和管理摄取管道。Elasticsearch将管道存储在集群状态。



## Prerequisites

- 具有ingest节点角色的节点处理管道处理。要使用  ingest pipelines，您的群集必须至少有一个具有 ingest pipelines, 角色的节点。对于繁重的摄取负载，我们建议创建专用的摄取节点。
- 如果启用了Elasticsearch安全功能，则必须具有*manage_pipeline*群集权限才能管理 pipelines。要使用Kibana的摄取管道功能，您还需要群集:  `cluster:monitor/nodes/info`  群集权限。
- 包括enrich处理器在内的管道需要额外的设置。 See [*Enrich your data*](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html).





## Create and manage pipelines

在Kibana中，打开主菜单，然后**Stack Management > Ingest Pipelines**。从列表视图中，您可以:

- View a list of your pipelines and drill down into details
- Edit or clone existing pipelines
- Delete pipelines

To create a pipeline, click **Create pipeline > New pipeline**. For an example tutorial, see [*Example: Parse logs*](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-log-format-example.html).



The **New pipeline from CSV** option lets you use a CSV to create an ingest pipeline that maps custom data to the [Elastic Common Schema (ECS)](https://www.elastic.co/guide/en/ecs/8.1). Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check [Map custom data to ECS](https://www.elastic.co/guide/en/ecs/8.1/ecs-converting.html).



You can also use the [ingest APIs](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-apis.html) to create and manage pipelines. The following [create pipeline API](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) request creates a pipeline containing two [`set`](https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html) processors followed by a [`lowercase`](https://www.elastic.co/guide/en/elasticsearch/reference/current/lowercase-processor.html) processor. The processors run sequentially in the order specified.

```console
PUT _ingest/pipeline/my-pipeline
{
  "description": "My optional pipeline description",
  "processors": [
    {
      "set": {
        "description": "My optional processor description",
        "field": "my-long-field",
        "value": 10
      }
    },
    {
      "set": {
        "description": "Set 'my-boolean-field' to true",
        "field": "my-boolean-field",
        "value": true
      }
    },
    {
      "lowercase": {
        "field": "my-keyword-field"
      }
    }
  ]
}
```

## Manage pipeline versions

When you create or update a pipeline, you can specify an optional `version` integer

创建或更新管道时，可以指定可选的版本整数。

。您可以将此版本号与  [`if_version`](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html#put-pipeline-api-query-params) parameter 一起使用，以有条件地更新管道。当指定*if_version*参数时，成功的更新会增加管道的版本

```
PUT _ingest/pipeline/my-pipeline-id
{
  "version": 1,
  "processors": [ ... ]
}
```

要使用API取消设置版本号，请在不指定版本参数的情况下替换或更新管道。

## Test a pipeline

在生产中使用管道之前，我们建议您使用示例文档对其进行测试。在Kibana中创建或编辑管道时，请单击 “添加文档”。在文档页签中，提供示例文档，然后单击运行管道。

You can also test pipelines using the [simulate pipeline API](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/simulate-pipeline-api.html). You can specify a configured pipeline in the request path. For example, the following request tests `my-pipeline`.

```
POST _ingest/pipeline/my-pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "my-keyword-field": "FOO"
      }
    },
    {
      "_source": {
        "my-keyword-field": "BAR"
      }
    }
  ]
}
```

Alternatively, you can specify a pipeline and its processors in the request body.

```console
POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "processors": [
      {
        "lowercase": {
          "field": "my-keyword-field"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "my-keyword-field": "FOO"
      }
    },
    {
      "_source": {
        "my-keyword-field": "BAR"
      }
    }
  ]
}
```

The API returns transformed documents:

```console-result
{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_type": "_doc",
        "_id": "_id",
        "_source": {
          "my-keyword-field": "foo"
        },
        "_ingest": {
          "timestamp": "2099-03-07T11:04:03.000Z"
        }
      }
    },
    {
      "doc": {
        "_index": "_index",
        "_type": "_doc",
        "_id": "_id",
        "_source": {
          "my-keyword-field": "bar"
        },
        "_ingest": {
          "timestamp": "2099-03-07T11:04:04.000Z"
        }
      }
    }
  ]
}
```

## Add a pipeline to an indexing request

Use the `pipeline` query parameter to apply a pipeline to documents in [individual](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-index_.html) or [bulk](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-bulk.html) indexing requests.

```console
POST my-data-stream/_doc?pipeline=my-pipeline
{
  "@timestamp": "2099-03-07T11:04:05.000Z",
  "my-keyword-field": "foo"
}

PUT my-data-stream/_bulk?pipeline=my-pipeline
{ "create":{ } }
{ "@timestamp": "2099-03-07T11:04:06.000Z", "my-keyword-field": "foo" }
{ "create":{ } }
{ "@timestamp": "2099-03-07T11:04:07.000Z", "my-keyword-field": "bar" }
```

You can also use the `pipeline` parameter with the [update by query](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-update-by-query.html) or [reindex](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-reindex.html) APIs.

```console
POST my-data-stream/_update_by_query?pipeline=my-pipeline

POST _reindex
{
  "source": {
    "index": "my-data-stream"
  },
  "dest": {
    "index": "my-new-data-stream",
    "op_type": "create",
    "pipeline": "my-pipeline"
  }
}
```





## Set a default pipeline

Use the [`index.default_pipeline`](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-modules.html#index-default-pipeline) index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no `pipeline` parameter is specified.

## Set a final pipeline

Use the [`index.final_pipeline`](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-modules.html#index-final-pipeline) index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified.



## Pipelines for Beats

To add an ingest pipeline to an Elastic Beat, specify the `pipeline` parameter under `output.elasticsearch` in `<BEAT_NAME>.yml`. For example, for Filebeat, you’d specify `pipeline` in `filebeat.yml`.

若要在Elastic Beat中添加一个Elastic pipeline，请在 <BEAT_NAME>.yml中指定output.elasticsearch下的pipeline参数。例如，对于Filebeat，您可以在filebeat.yml中指定管道。



```yaml
output.elasticsearch:
  hosts: ["localhost:9200"]
  pipeline: my-pipeline
```





## Access source fields in a processor

Processors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following `set` processor accesses `my-long-field`.

处理器具有对传入文档的源字段的读写权限。要访问处理器中的 field key，请使用其字段名称。下面的set处理器访问my-long-field。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "field": "my-long-field",
        "value": 10
      }
    }
  ]
}
```

You can also prepend the `_source` prefix.

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "field": "_source.my-long-field",
        "value": 10
      }
    }
  ]
}
```

使用点表示法访问对象字段。

注意：

If your document contains flattened objects, use the [`dot_expander`](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/dot-expand-processor.html) processor to expand them first. Other ingest processors cannot access flattened objects.

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "dot_expander": {
        "description": "Expand 'my-object-field.my-property'",
        "field": "my-object-field.my-property"
      }
    },
    {
      "set": {
        "description": "Set 'my-object-field.my-property' to 10",
        "field": "my-object-field.my-property",
        "value": 10
      }
    }
  ]
}
```



Several processor parameters support [Mustache](https://mustache.github.io/) template snippets.

支持大胡子模板引擎

 To access field values in a template snippet, enclose the field name in triple curly brackets:`{{{field-name}}}`. You can use template snippets to dynamically set field names.



## Access metadata fields in a processor

Processors can access the following metadata fields by name:

- `_index`
- `_id`
- `_routing`
- `_dynamic_templates`

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Set '_routing' to 'geoip.country_iso_code' value",
        "field": "_routing",
        "value": "{{{geoip.country_iso_code}}}"
      }
    }
  ]
}
```

使用大胡子模板片段访问元数据字段值。例如 {{{_routing }}} 检索文档的路由值。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Use geo_point dynamic template for address field",
        "field": "_dynamic_templates",
        "value": {
          "address": "geo_point"
        }
      }
    }
  ]
}
```

The set processor above tells ES to use the dynamic template named `geo_point` for the field `address` if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field `address` if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request.

如果索引的映射中尚未定义此字段，则上面的set处理器会告诉ES将名为geo_point的动态模板用于字段地址。如果已在批量请求中定义，则此处理器将覆盖字段地址的动态模板，但对批量请求中定义的其他动态模板没有影响。



## Handling pipeline failures

管道的处理器按顺序运行。默认情况下，当这些处理器之一出现故障或遇到错误时，管道处理将停止。
要忽略处理器故障并运行管道的其余处理器，请将ignore_failure设置为true。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "rename": {
        "description": "Rename 'provider' to 'cloud.provider'",
        "field": "provider",
        "target_field": "cloud.provider",
        "ignore_failure": true
      }
    }
  ]
}
```

使用*on_failure*参数指定处理器故障后立即运行的处理器列表。如果指定了on_failure，则即使*on_failure*配置为空，Elasticsearch也会随后运行管道的其余处理器。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "rename": {
        "description": "Rename 'provider' to 'cloud.provider'",
        "field": "provider",
        "target_field": "cloud.provider",
        "on_failure": [
          {
            "set": {
              "description": "Set 'error.message'",
              "field": "error.message",
              "value": "Field 'provider' does not exist. Cannot rename to 'cloud.provider'",
              "override": false,
              "on_failure": [
                {
                  "set": {
                    "description": "Set 'error.message.multi'",
                    "field": "error.message.multi",
                    "value": "Document encountered multiple ingest errors",
                    "override": true
                  }
                }
              ]
            }
          }
        ]
      }
    }
  ]
}
```



You can also specify `on_failure` for a pipeline. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors.

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [ ... ],
  "on_failure": [
    {
      "set": {
        "description": "Index document to 'failed-<index>'",
        "field": "_index",
        "value": "failed-{{{ _index }}}"
      }
    }
  ]
}
```

Additional information about the pipeline failure may be available in the document metadata fields `on_failure_message`, `on_failure_processor_type`, `on_failure_processor_tag`, and `on_failure_pipeline`. These fields are accessible only from within an `on_failure` block.

以下示例使用元数据字段在文档中包含有关管道故障的信息。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [ ... ],
  "on_failure": [
    {
      "set": {
        "description": "Record error information",
        "field": "error_information",
        "value": "Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}"
      }
    }
  ]
}
```



## Conditionally run a processor

Each processor supports an optional `if` condition, written as a [Painless script](https://www.elastic.co/guide/en/elasticsearch/painless/7.16/painless-guide.html). If provided, the processor only runs when the `if` condition is `true`.

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents with 'network.name' of 'Guest'",
        "if": "ctx?.network?.name == 'Guest'"
      }
    }
  ]
}
```

If the [`script.painless.regex.enabled`](https://www.elastic.co/guide/en/elasticsearch/reference/7.16/circuit-breaker.html#script-painless-regex-enabled) cluster setting is enabled, you can use regular expressions in your `if` condition scripts. For supported syntax, see [Painless regular expressions](https://www.elastic.co/guide/en/elasticsearch/painless/7.16/painless-regexes.html).



If possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds.

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "If 'url.scheme' is 'http', set 'url.insecure' to true",
        "if": "ctx.url?.scheme =~ /^http[^s]/",
        "field": "url.insecure",
        "value": true
      }
    }
  ]
}
```

您必须将if条件指定为单行上的有效JSON。但是，您可以使用Kibana控制台的三引号语法来编写和调试更大的脚本。

```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that don't contain 'prod' tag",
        "if": """
            Collection tags = ctx.tags;
            if(tags != null){
              for (String tag : tags) {
                if (tag.toLowerCase().contains('prod')) {
                  return false;
                }
              }
            }
            return true;
        """
      }
    }
  ]
}
```

You can also specify a [stored script](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/modules-scripting-stored-scripts.html) as the `if` condition.

```console
PUT _scripts/my-prod-tag-script
{
  "script": {
    "lang": "painless",
    "source": """
      Collection tags = ctx.tags;
      if(tags != null){
        for (String tag : tags) {
          if (tag.toLowerCase().contains('prod')) {
            return false;
          }
        }
      }
      return true;
    """
  }
}

PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that don't contain 'prod' tag",
        "if": { "id": "my-prod-tag-script" }
      }
    }
  ]
}
```

传入文档通常包含对象字段。如果处理器脚本试图访问父对象不存在的字段

如果处理器脚本尝试访问父对象不存在的字段，则Elasticsearch将返回NullPointerException。

要避免这些异常，请使用null safe运算符，例如 `?.`，并将脚本编写为null safe。

```
 `ctx.network?.name.equalsIgnoreCase('Guest')` is not null safe. `ctx.network?.name` can return null. 

Rewrite the script as `'Guest'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.

```



```console
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that contain 'network.name' of 'Guest'",
        "if": "ctx.network?.name != null && ctx.network.name.contains('Guest')"
      }
    }
  ]
}
```

## Conditionally apply pipelines

将if条件与管道处理器结合使用，以根据您的条件将其他管道应用于文档。您可以将此管道用作用于配置多个数据流或索引的索引模板中的默认管道。

```console
PUT _ingest/pipeline/one-pipeline-to-rule-them-all
{
  "processors": [
    {
      "pipeline": {
        "description": "If 'service.name' is 'apache_httpd', use 'httpd_pipeline'",
        "if": "ctx.service?.name == 'apache_httpd'",
        "name": "httpd_pipeline"
      }
    },
    {
      "pipeline": {
        "description": "If 'service.name' is 'syslog', use 'syslog_pipeline'",
        "if": "ctx.service?.name == 'syslog'",
        "name": "syslog_pipeline"
      }
    },
    {
      "fail": {
        "description": "If 'service.name' is not 'apache_httpd' or 'syslog', return a failure message",
        "if": "ctx.service?.name != 'apache_httpd' && ctx.service?.name != 'syslog'",
        "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"
      }
    }
  ]
}
```

### Get pipeline usage statistics

Use the [node stats](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/cluster-nodes-stats.html) API to get global and per-pipeline ingest statistics. 

Use these stats to determine which pipelines run most frequently or spend the most time processing.

```console
GET _nodes/stats/ingest?filter_path=nodes.*.ingest
```
